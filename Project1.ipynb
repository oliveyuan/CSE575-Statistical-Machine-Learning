{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6wXN8oWOcLfX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c0e68c2-68e9-42e1-c624-7541bb1974d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /drive; to attempt to forcibly remount, call drive.mount(\"/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "#CSE 575 Group 2 Project 1\n",
        "#Summer 2024\n",
        "#Prof. Samira Ghayekhloo\n",
        "#Project Team: Arjun Dadhwal, Ching-Chun Yuan, Jeffrey Li\n",
        "\n",
        "import scipy.io # Library used to load the datasets.\n",
        "import numpy as np # Library used for various operations involving arrays of various dimensions.\n",
        "from google.colab import drive #This was used to import the files stored on Google Drive. Feel free to remove these two lines.\n",
        "drive.mount('/drive')\n",
        "\n",
        "\n",
        "#Given tasks:\n",
        "    # 1. Write code to extract features for both training set and testing set.\n",
        "    # 2. Write code to implement the Naive Bayes Classifier and use it produce a predicted label for each testing sample.\n",
        "    # 3. Write code to compute the classification accuracy, for the Naive Bayes Classifier\n",
        "    # 4. Write a short report summarizing the results, including the final classification accuracy.\n",
        "\n",
        "\n",
        "#Loading the datasets.\n",
        "#Note: Feel free to change the path to the files based on your setup.\n",
        "\n",
        "train_0_file = scipy.io.loadmat('/drive/MyDrive/test/train_0_img.mat') #Loading the training data set with digit 0 samples.\n",
        "train_1_file = scipy.io.loadmat('/drive/MyDrive/test/train_1_img.mat') #Loading the training data set with digit 1 samples.\n",
        "test_0_file = scipy.io.loadmat('/drive/MyDrive/test/test_0_img.mat')  #Loading the testing data set with digit 0 samples.\n",
        "test_1_file = scipy.io.loadmat('/drive/MyDrive/test/test_1_img.mat') #Loading the testing data set with digit 1 samples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kak7h2K1th0L"
      },
      "outputs": [],
      "source": [
        "#The below variables extract the relevant data containing arrays from the datasets which were of \"dict type\", using the \"target_img\" key.\n",
        "\n",
        "train_data_0 = train_0_file['target_img']\n",
        "train_data_1 = train_1_file['target_img']\n",
        "test_data_0 = test_0_file['target_img']\n",
        "test_data_1 = test_1_file['target_img']\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VUcjy2jp_0l3"
      },
      "outputs": [],
      "source": [
        "#The code below accomplishes the given Task 1.\n",
        "\n",
        "#This function extracts 2 features from the given samples of the dataset passed through parameter. The samples are images represented by a 2-D array of pixels with brightness values.\n",
        "#It then returns a Feature array containing the value of the features for each sample.\n",
        "# Feature 1 = Mean of the brightness of the pixels in each sample.\n",
        "# Feature 2 = Standard deviation of the brightness of the pixels in each sample.\n",
        "\n",
        "def extractFeature(images):\n",
        "    num_images = images.shape[2] # The size of the third dimension of the data array gives the total number of sample images.\n",
        "    features = np.zeros((num_images, 2)) # Create the 2-D array to store the value of the features for each sample.\n",
        "    for i in range(num_images):\n",
        "        img = images[:, :, i] # Extract the brightness values of all the pixels of the current image.\n",
        "        features[i, 0] = np.mean(img) # Store the first feature, the mean brightness.\n",
        "        features[i, 1] = np.std(img) # Store the second feature, the standard deviation.\n",
        "    return features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eBP1oCnY_8sq"
      },
      "outputs": [],
      "source": [
        "#Extract the features from the training datasets, for digit 0 and digit 1.\n",
        "\n",
        "train_0feat = extractFeature(train_data_0)\n",
        "train_1feat = extractFeature(train_data_1)\n",
        "\n",
        "#Extract the features from the testing datasets, for digit 0 and digit 1.\n",
        "\n",
        "test_0feat = extractFeature(test_data_0)\n",
        "test_1feat = extractFeature(test_data_1)\n",
        "\n",
        "#Calculate the total number of samples from the training dataset for each digit. This will be total number of that particular digit in the sample.\n",
        "\n",
        "size_of_train_img0  = train_0feat.shape[0]\n",
        "size_of_train_img1  = train_1feat.shape[0]\n",
        "\n",
        "#Calculate the prior probabilities for each digit in the training set. This will be useful later on in the Naive Bayes function.\n",
        "#In Naive Bayes, we can predict the Prior probability with the formula: P(Y) = (The number of samples belonging to Y class) / Total number of samples)\n",
        "\n",
        "prior0 = (size_of_train_img0) / (size_of_train_img0 + size_of_train_img1) #The total size of training dataset is the sum of the sizes of both the dataset.\n",
        "prior1 = (size_of_train_img1) / (size_of_train_img0 + size_of_train_img1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FGX3-htJ0CgJ"
      },
      "outputs": [],
      "source": [
        "#This function takes in features of the training set of a digit as its input and uses that to estimate the parameters for the 2-D normal distribution for that digit.\n",
        "#The estimated parameters are the Mean vector for both the features in the dataset and the Covariance matrix for both the features in the dataset.\n",
        "\n",
        "def getParameters(features):\n",
        "    mean = np.mean(features, axis=0) # Calculate the mean vector of the features.\n",
        "    cov_matrix = np.cov(features, rowvar = False) # Calculate the Covariance.\n",
        "    return mean, cov_matrix\n",
        "\n",
        "#The estimated paramters for the training sets for both the digits, 0 and 1.\n",
        "\n",
        "train_0_mean, train_0_covMatrix = getParameters(train_0feat)\n",
        "train_1_mean, train_1_covMatrix = getParameters(train_1feat)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#The 2D Gaussian (Normal) Distribution is used to calculate the Likelihood P(x|Y),where x is the feature and Y is the label.\n",
        "#The likelihood is useful for later on in the Naive Bayes classifier for calculating the Posterior Probability which will be used for predicting the class labels.\n",
        "#Formula: p(x) = (1/( (2pi)^(d/2) * sqrt(covariance matrix))) * exp(-1/2(x - mean)^t * inverse(covariance matrix) * (x - mean))\n",
        "\n",
        "\n",
        "#x is the current testing set sample. mu the mean vector and sigma the covariance matrix which are the earlier calculated parameters for the given digit from the normal distributions of its training set.\n",
        "def Gaussian2D(x, mu, sigma):\n",
        "    d = x.shape[1]  #d is the number of features, which will be 2.\n",
        "    diff = x - mu # Calculate the difference between the current sample and\n",
        "    inv_sigma = np.linalg.inv(sigma) #Calculate the inverce of the Covariance matrix.\n",
        "    norm_const = 1.0 / (np.sqrt((2 * np.pi) ** d * np.linalg.det(sigma))) # Calculate the denominator for the Gaussian formula.\n",
        "    result = norm_const * np.exp(-0.5 * np.sum(diff @ inv_sigma * diff, axis=1))\n",
        "    return result"
      ],
      "metadata": {
        "id": "502nmIZWZMYf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mHXiTEQUAXoD"
      },
      "outputs": [],
      "source": [
        "#A Gaussian Naives Bayes classifier. Takes in the features for each sample in the testing data and using previously established parameters\n",
        "#predicts the label for the sample.\n",
        "\n",
        "def naiveBayes(test_samples, mean_0, cov_7, mean_8, cov_8, prior0, prior1):\n",
        "\n",
        "    #Calculate posterior probability for both the labels 0 and 1.\n",
        "    #Posterior Probability (P(Y|X)) = Likelihood (P(X|Y)) * Prior P(Y) where Y is the label, X is the features vector for the sample.\n",
        "    #The likelihood is calculated using the Gaussian Distribution Formula using the earlier created function.\n",
        "\n",
        "    posterior0 = Gaussian2D(test_samples, mean_0, cov_7) * prior0 #For digit 0. Function uses the features vector and the estimated training set parameters along with prior.\n",
        "    posterior1 = Gaussian2D(test_samples, mean_8, cov_8) * prior1 #For digit 0. Function uses the features vector and the estimated training set parameters along with prior.\n",
        "\n",
        "\n",
        "    #It returns true (1) if the posterior for digit 0 is higher, which means that 0 is the predicted digit.\n",
        "\n",
        "    return posterior0 > posterior1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "94gz6FSSyoU6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03fbbe06-8c8b-42f4-e89e-113f25112e97"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy for digit 0: 97.45%\n",
            "Accuracy for digit 1: 96.39%\n"
          ]
        }
      ],
      "source": [
        "test_features = np.vstack([test_0feat, test_1feat]) #Combine the feature values for the samples in the testing data sets for both digits into one stack.\n",
        "\n",
        "#Generate the labels for the testing set for comparision with the predictions. Since we already know the digits for the testing set, generate the labels based on the number of samples.\n",
        "#True(1) if the label is digit 0 or False (0) if the label is digit 1.\n",
        "test_labels = np.concatenate([np.ones(test_0feat.shape[0]), np.zeros(test_1feat.shape[0])])\n",
        "\n",
        "#Call the Naive Bayes function to predict the labels from testing data set and store the result in the predictions array.\n",
        "#Note: Predictions store whether that sample should be digit 0 or not. So digit 0 predictions will be stored as 1 (true).\n",
        "\n",
        "predictions = naiveBayes(test_features, train_0_mean, train_0_covMatrix, train_1_mean, train_1_covMatrix, prior0, prior1)\n",
        "\n",
        "#This functions takes in the predictions made by the Naive Bayes classifier and compares them with the target labels in the test set based on the target class.\n",
        "#Note: The labels for digit 0 are stored as 1 (true).\n",
        "\n",
        "def accuracy(predictions, labels, isDigit0):\n",
        "    target_mask = labels == isDigit0 #Get the labels for the target digit that we are trying to find.\n",
        "    correct = np.sum(predictions[target_mask] == labels[target_mask]) #Count the total number of correct predictions i.e. prediction is same as the target class from the training set.\n",
        "    total = np.sum(target_mask) # Get the total number of the samples for which the labels was checked.\n",
        "    return correct/total # Return the accuracy as the ratio of correctly predicted labels to the total number of labels.\n",
        "\n",
        "#Calculate the classification accuracy for the predictions made for each digit.\n",
        "\n",
        "accuracy0 = accuracy(predictions, test_labels, 1)\n",
        "accuracy1 = accuracy(predictions, test_labels, 0)\n",
        "\n",
        "#Print the results for the classification accuracy for each digit.\n",
        "\n",
        "print(f'Accuracy for digit 0: {accuracy0 * 100:.2f}%')\n",
        "print(f'Accuracy for digit 1: {accuracy1 * 100:.2f}%')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}